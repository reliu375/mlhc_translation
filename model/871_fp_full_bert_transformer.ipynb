{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"full_bert_transformer_renbins_version.ipynb","provenance":[{"file_id":"1ehg1RNDanQG59sU5inCyqSqxZb8abgAn","timestamp":1620578586285},{"file_id":"1p1HI66I6QY0mbN4ufalN5BId8oBA-IaC","timestamp":1620518894919}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ca1321c099c544778e3fab0347ae542f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_036727e8bea943c49e1da96774e30e42","IPY_MODEL_8a0a54bc75e54111968232ddbf63f7dc"],"layout":"IPY_MODEL_72aabe0b5c464852b19a8a12672ae1a1"}},"036727e8bea943c49e1da96774e30e42":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_413ea242de4d49858ff3b1e0b4e79097","max":385,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f9c431f5fed14c66a4043f45a57483e7","value":385}},"8a0a54bc75e54111968232ddbf63f7dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0869307b3ff647308615abdc47fcba9b","placeholder":"​","style":"IPY_MODEL_f8dee9cf2e484d7d92abec2e4c1140fb","value":" 385/385 [00:00&lt;00:00, 993B/s]"}},"72aabe0b5c464852b19a8a12672ae1a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"413ea242de4d49858ff3b1e0b4e79097":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9c431f5fed14c66a4043f45a57483e7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"0869307b3ff647308615abdc47fcba9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8dee9cf2e484d7d92abec2e4c1140fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"23c24c0aa9144ffaa89e5e11015e41a1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_75fee5a97f3d41a38727c87ff944ea65","IPY_MODEL_54765fbcc95c468384805ac89e6caacc"],"layout":"IPY_MODEL_98399d66718e4b3c85e6b41845b71521"}},"75fee5a97f3d41a38727c87ff944ea65":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_e0eadeb3acdf4eeba7ea58dc6d111702","max":213450,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2fac424e283f4ad9a5013022ec4822ba","value":213450}},"54765fbcc95c468384805ac89e6caacc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d471ce9533dc47cba9e2c9ad97f7bd71","placeholder":"​","style":"IPY_MODEL_f121edf7e561473591acc095bfc1bb57","value":" 213k/213k [00:01&lt;00:00, 189kB/s]"}},"98399d66718e4b3c85e6b41845b71521":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0eadeb3acdf4eeba7ea58dc6d111702":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fac424e283f4ad9a5013022ec4822ba":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"d471ce9533dc47cba9e2c9ad97f7bd71":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f121edf7e561473591acc095bfc1bb57":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a25e878072da484981e4e0d561b742d1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6176f70d2c6f448c9ef14534f4e107a4","IPY_MODEL_0a5611343f9544c09d0ec6eed12928dc"],"layout":"IPY_MODEL_d9f7774ce798415699d3689964974811"}},"6176f70d2c6f448c9ef14534f4e107a4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_42f23081243447558b7bf8489476487a","max":435778770,"min":0,"orientation":"horizontal","style":"IPY_MODEL_773f3e21f67c49c08835032f41666c18","value":435778770}},"0a5611343f9544c09d0ec6eed12928dc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_39fa0b85dc014982834974463004c2f3","placeholder":"​","style":"IPY_MODEL_213bb7ef2b38447c83a2abe32098dc05","value":" 436M/436M [00:07&lt;00:00, 54.8MB/s]"}},"d9f7774ce798415699d3689964974811":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42f23081243447558b7bf8489476487a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"773f3e21f67c49c08835032f41666c18":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"39fa0b85dc014982834974463004c2f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"213bb7ef2b38447c83a2abe32098dc05":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ATE92Xqa-K3X","executionInfo":{"elapsed":10521,"status":"ok","timestamp":1620943951863,"user":{"displayName":"Renbin Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDg7JBPfGJg4OqSmd6Xvej6r-SRxhyxgILgsfW=s64","userId":"05750027745545487753"},"user_tz":420},"outputId":"70bef4a3-d2b3-4cce-94cd-1f26fefec0a5"},"source":["!pip -q install transformers\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import math\n","import torch.nn.functional as F\n","import copy\n","import json\n","import time\n","from transformers import AutoTokenizer, AutoModel"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 2.3MB 8.0MB/s \n","\u001b[K     |████████████████████████████████| 3.3MB 54.3MB/s \n","\u001b[K     |████████████████████████████████| 901kB 58.5MB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6wlR2Dl--K3b"},"source":["# Citations\n","# https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec\n","# https://pytorch.org/tutorials/beginner/transformer_tutorial.html"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CHQT1eyW-OoI","executionInfo":{"elapsed":28157,"status":"ok","timestamp":1620943969524,"user":{"displayName":"Renbin Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDg7JBPfGJg4OqSmd6Xvej6r-SRxhyxgILgsfW=s64","userId":"05750027745545487753"},"user_tz":420},"outputId":"d950d02d-9c92-4ce3-f25b-6f1b95172dd3"},"source":["from google.colab import drive \n","drive.mount(\"/content/drive\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jZW1XFgA-K3b"},"source":["device = \"cuda\" # change to \"cuda\" when moved to google colab?"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yVG-EIeN-K3c"},"source":["### Preprocessing ### "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-p3PxdU1-K3c"},"source":["# Constants for state of data processing\n","BLANK = 0\n","ORIGINAL = 1\n","REPHRASED = 2\n","TRANSLATED = 3\n","\n","def process_data(filename, test=False):\n","    state = ORIGINAL\n","    original_sents, translated_sents, rephrased_sents = [], [], []\n","    file = open(filename, 'r')\n","    while True:\n","        line = file.readline()\n","        # print(line, len(line))\n","        if not line: #EOF\n","            break \n","        if not test:\n","            if state == ORIGINAL: # Original sentence\n","                original_sents.append(line.strip())\n","                state = TRANSLATED\n","            elif state == TRANSLATED: # Translated sentence\n","                translated_sents.append(line.strip())\n","                state = BLANK\n","            elif state == BLANK: # Blank line\n","                assert line == '\\n' # If assertion fails, something wrong with the file format\n","                state = ORIGINAL\n","            else:\n","                raise ValueError('Unexpected state encountered.')\n","        else:\n","            if state == ORIGINAL: # Original sentence\n","                original_sents.append(line.strip())\n","                state = REPHRASED\n","            elif state == REPHRASED: # Translated sentence\n","                rephrased_sents.append(line.strip())\n","                state = TRANSLATED\n","            elif state == TRANSLATED: # Translated sentence\n","                translated_sents.append(line.strip())\n","                state = BLANK\n","            elif state == BLANK: # Blank line\n","                assert line == '\\n' # If assertion fails, something wrong with the file format\n","                state = ORIGINAL\n","            else:\n","                raise ValueError('Unexpected state encountered.')\n","   \n","    assert len(original_sents) == len(translated_sents) # all sents should be paired\n","    if test:\n","        assert len(original_sents) == len(rephrased_sents)\n","\n","    return {'original': original_sents, 'translated': translated_sents, 'rephrased': rephrased_sents}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5wGtoVgb-K3d","executionInfo":{"elapsed":29794,"status":"ok","timestamp":1620943971195,"user":{"displayName":"Renbin Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDg7JBPfGJg4OqSmd6Xvej6r-SRxhyxgILgsfW=s64","userId":"05750027745545487753"},"user_tz":420},"outputId":"f0c2c134-98e9-4363-c77c-bcc6a36ebf32"},"source":["%%time\n","train = process_data('/content/drive/MyDrive/MedLane/train(12809)_new.txt')\n","test = process_data('/content/drive/MyDrive/MedLane/test(2030)_new.txt', test=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["CPU times: user 31.6 ms, sys: 11.2 ms, total: 42.8 ms\n","Wall time: 1.65 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RzhIVL1kq4sa"},"source":["test['translated'] = [sentence[2:] for sentence in test['translated']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"Q5ZdnG5frIqh","executionInfo":{"elapsed":29761,"status":"ok","timestamp":1620943971197,"user":{"displayName":"Renbin Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDg7JBPfGJg4OqSmd6Xvej6r-SRxhyxgILgsfW=s64","userId":"05750027745545487753"},"user_tz":420},"outputId":"8c132001-10e8-4080-e1f8-2192f684c48e"},"source":["train['translated'][1222]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'She was also noted to have some bleeding around her epidural site .'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"7fM0koBMA0p8"},"source":["from collections import Counter"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["ca1321c099c544778e3fab0347ae542f","036727e8bea943c49e1da96774e30e42","8a0a54bc75e54111968232ddbf63f7dc","72aabe0b5c464852b19a8a12672ae1a1","413ea242de4d49858ff3b1e0b4e79097","f9c431f5fed14c66a4043f45a57483e7","0869307b3ff647308615abdc47fcba9b","f8dee9cf2e484d7d92abec2e4c1140fb","23c24c0aa9144ffaa89e5e11015e41a1","75fee5a97f3d41a38727c87ff944ea65","54765fbcc95c468384805ac89e6caacc","98399d66718e4b3c85e6b41845b71521","e0eadeb3acdf4eeba7ea58dc6d111702","2fac424e283f4ad9a5013022ec4822ba","d471ce9533dc47cba9e2c9ad97f7bd71","f121edf7e561473591acc095bfc1bb57"]},"id":"aV6vIU44-K3d","executionInfo":{"elapsed":36606,"status":"ok","timestamp":1620943978112,"user":{"displayName":"Renbin Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDg7JBPfGJg4OqSmd6Xvej6r-SRxhyxgILgsfW=s64","userId":"05750027745545487753"},"user_tz":420},"outputId":"01c75878-1450-48e5-dabc-0dfff9efe5e5"},"source":["%%time\n","bert_tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n","\n","def tokenize_and_build_vocab(corpus, tokenization_func, special_tokens, min_freq = 0):\n","    vocab_counter = Counter()\n","    tokenized_sent = []\n","    for (i, sent) in enumerate(corpus):\n","        if i % 200 == 0: \n","          print('reached corpus sentence ' + str(i))\n","        token_list = tokenization_func(sent)\n","        tokenized_sent.append(token_list)\n","        vocab_counter.update(token_list)\n","        \n","    vocab_to_ix = {vocab: ix + len(special_tokens) for ix, (vocab, freq) in enumerate(vocab_counter.most_common()) if freq >= min_freq}\n","    \n","    for tok, ix in special_tokens.items():\n","        vocab_to_ix[tok] = ix\n","        \n","    return tokenized_sent, dict(vocab_counter.most_common()), vocab_to_ix\n","\n","\n","tokenization_func = lambda x: bert_tokenizer.tokenize(x)\n","special_tokens = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n","print('created tokenizer')\n","\n","train_src_sent, _, _ = tokenize_and_build_vocab(train['original'], tokenization_func, special_tokens)\n","train_tgt_sent, _, _ = tokenize_and_build_vocab(train['translated'], tokenization_func, special_tokens)\n","\n","test_src_sent, _, _ = tokenize_and_build_vocab(test['original'], tokenization_func, special_tokens)\n","test_tgt_sent, _, _ = tokenize_and_build_vocab(test['translated'], tokenization_func, special_tokens)\n"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ca1321c099c544778e3fab0347ae542f","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=385.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"23c24c0aa9144ffaa89e5e11015e41a1","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=213450.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","created tokenizer\n","reached corpus sentence 0\n","reached corpus sentence 200\n","reached corpus sentence 400\n","reached corpus sentence 600\n","reached corpus sentence 800\n","reached corpus sentence 1000\n","reached corpus sentence 1200\n","reached corpus sentence 1400\n","reached corpus sentence 1600\n","reached corpus sentence 1800\n","reached corpus sentence 2000\n","reached corpus sentence 2200\n","reached corpus sentence 2400\n","reached corpus sentence 2600\n","reached corpus sentence 2800\n","reached corpus sentence 3000\n","reached corpus sentence 3200\n","reached corpus sentence 3400\n","reached corpus sentence 3600\n","reached corpus sentence 3800\n","reached corpus sentence 4000\n","reached corpus sentence 4200\n","reached corpus sentence 4400\n","reached corpus sentence 4600\n","reached corpus sentence 4800\n","reached corpus sentence 5000\n","reached corpus sentence 5200\n","reached corpus sentence 5400\n","reached corpus sentence 5600\n","reached corpus sentence 5800\n","reached corpus sentence 6000\n","reached corpus sentence 6200\n","reached corpus sentence 6400\n","reached corpus sentence 6600\n","reached corpus sentence 6800\n","reached corpus sentence 7000\n","reached corpus sentence 7200\n","reached corpus sentence 7400\n","reached corpus sentence 7600\n","reached corpus sentence 7800\n","reached corpus sentence 8000\n","reached corpus sentence 8200\n","reached corpus sentence 8400\n","reached corpus sentence 8600\n","reached corpus sentence 8800\n","reached corpus sentence 9000\n","reached corpus sentence 9200\n","reached corpus sentence 9400\n","reached corpus sentence 9600\n","reached corpus sentence 9800\n","reached corpus sentence 10000\n","reached corpus sentence 10200\n","reached corpus sentence 10400\n","reached corpus sentence 10600\n","reached corpus sentence 10800\n","reached corpus sentence 11000\n","reached corpus sentence 11200\n","reached corpus sentence 11400\n","reached corpus sentence 11600\n","reached corpus sentence 11800\n","reached corpus sentence 12000\n","reached corpus sentence 12200\n","reached corpus sentence 12400\n","reached corpus sentence 12600\n","reached corpus sentence 12800\n","reached corpus sentence 0\n","reached corpus sentence 200\n","reached corpus sentence 400\n","reached corpus sentence 600\n","reached corpus sentence 800\n","reached corpus sentence 1000\n","reached corpus sentence 1200\n","reached corpus sentence 1400\n","reached corpus sentence 1600\n","reached corpus sentence 1800\n","reached corpus sentence 2000\n","reached corpus sentence 2200\n","reached corpus sentence 2400\n","reached corpus sentence 2600\n","reached corpus sentence 2800\n","reached corpus sentence 3000\n","reached corpus sentence 3200\n","reached corpus sentence 3400\n","reached corpus sentence 3600\n","reached corpus sentence 3800\n","reached corpus sentence 4000\n","reached corpus sentence 4200\n","reached corpus sentence 4400\n","reached corpus sentence 4600\n","reached corpus sentence 4800\n","reached corpus sentence 5000\n","reached corpus sentence 5200\n","reached corpus sentence 5400\n","reached corpus sentence 5600\n","reached corpus sentence 5800\n","reached corpus sentence 6000\n","reached corpus sentence 6200\n","reached corpus sentence 6400\n","reached corpus sentence 6600\n","reached corpus sentence 6800\n","reached corpus sentence 7000\n","reached corpus sentence 7200\n","reached corpus sentence 7400\n","reached corpus sentence 7600\n","reached corpus sentence 7800\n","reached corpus sentence 8000\n","reached corpus sentence 8200\n","reached corpus sentence 8400\n","reached corpus sentence 8600\n","reached corpus sentence 8800\n","reached corpus sentence 9000\n","reached corpus sentence 9200\n","reached corpus sentence 9400\n","reached corpus sentence 9600\n","reached corpus sentence 9800\n","reached corpus sentence 10000\n","reached corpus sentence 10200\n","reached corpus sentence 10400\n","reached corpus sentence 10600\n","reached corpus sentence 10800\n","reached corpus sentence 11000\n","reached corpus sentence 11200\n","reached corpus sentence 11400\n","reached corpus sentence 11600\n","reached corpus sentence 11800\n","reached corpus sentence 12000\n","reached corpus sentence 12200\n","reached corpus sentence 12400\n","reached corpus sentence 12600\n","reached corpus sentence 12800\n","reached corpus sentence 0\n","reached corpus sentence 200\n","reached corpus sentence 400\n","reached corpus sentence 600\n","reached corpus sentence 800\n","reached corpus sentence 1000\n","reached corpus sentence 1200\n","reached corpus sentence 1400\n","reached corpus sentence 1600\n","reached corpus sentence 1800\n","reached corpus sentence 2000\n","reached corpus sentence 0\n","reached corpus sentence 200\n","reached corpus sentence 400\n","reached corpus sentence 600\n","reached corpus sentence 800\n","reached corpus sentence 1000\n","reached corpus sentence 1200\n","reached corpus sentence 1400\n","reached corpus sentence 1600\n","reached corpus sentence 1800\n","reached corpus sentence 2000\n","CPU times: user 4.67 s, sys: 269 ms, total: 4.94 s\n","Wall time: 6.86 s\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JezAdHFWdrrh","executionInfo":{"elapsed":36597,"status":"ok","timestamp":1620943978113,"user":{"displayName":"Renbin Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDg7JBPfGJg4OqSmd6Xvej6r-SRxhyxgILgsfW=s64","userId":"05750027745545487753"},"user_tz":420},"outputId":"8079bd34-eb44-4dfb-b870-f508171e0bc3"},"source":["test_tgt_sent[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['we',\n"," '##ane',\n"," '##d',\n"," 'off',\n"," 'vent',\n"," 'to',\n"," 'c',\n"," '##pa',\n"," '##p',\n"," '[',\n"," 'continuous',\n"," 'positive',\n"," 'air',\n"," '##way',\n"," 'pressure',\n"," ']',\n"," 'and',\n"," 'was',\n"," 'ex',\n"," '##tub',\n"," '##ated',\n"," 'in',\n"," 'the',\n"," 'afternoon',\n"," 'on',\n"," '9',\n"," '-',\n"," '2',\n"," 'by',\n"," 'the',\n"," 'lung',\n"," 'specialist',\n"," 'team',\n"," '.']"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"4vTb8c96-K3e"},"source":["from torch.utils import data\n","\n","# These IDs are reserved.\n","# PAD_INDEX = 0\n","# UNK_INDEX = 1\n","# SOS_INDEX = 2\n","# EOS_INDEX = 3\n","\n","class MTDatasetForBERT(data.Dataset):\n","    def __init__(self, bert_tokenizer, src_sents, tgt_sents, num_samples=1.):\n","        self.src_sents = src_sents[:int(len(src_sents) * num_samples)]\n","        self.tgt_sents = tgt_sents[:int(len(src_sents) * num_samples)]\n","        self.bert_tokenizer = bert_tokenizer\n","        \n","        self.max_src_seq_len = 201\n","        self.max_tgt_seq_len = 201\n","\n","        self.SOS = self.bert_tokenizer.convert_tokens_to_ids('[CLS]')\n","        self.EOS = self.bert_tokenizer.convert_tokens_to_ids('[SEP]')\n","        self.UNK = self.bert_tokenizer.convert_tokens_to_ids('[UNK]')\n","        self.PAD = self.bert_tokenizer.convert_tokens_to_ids('[PAD]')\n","        \n","        assert len(self.src_sents) == len(self.tgt_sents)\n","        \n","    def __getitem__(self, index):\n","        src_sent = self.src_sents[index]\n","        src_len = len(src_sent) + 2 # including <SOS> <EOS>\n","        src_ids = [token for token in self.bert_tokenizer.convert_tokens_to_ids(src_sent)]\n","        \n","        src_id = ([self.SOS] + src_ids + [self.EOS] + [self.PAD] *\n","              (self.max_src_seq_len - src_len)) # Padding\n","        \n","        tgt_sent = self.tgt_sents[index]\n","        tgt_len = len(tgt_sent) + 2 # including <SOS> <EOS>\n","        tgt_ids = [token for token in self.bert_tokenizer.convert_tokens_to_ids(tgt_sent)]\n","                \n","        tgt_id = ([self.SOS] + tgt_ids + [self.EOS] + [self.PAD] *\n","              (self.max_tgt_seq_len - tgt_len))       \n","        \n","        return torch.tensor(src_id), src_len, torch.tensor(tgt_id), tgt_len\n","    \n","    def __len__(self):\n","        return len(self.src_sents)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eyyGZ14_-K3f","executionInfo":{"elapsed":36585,"status":"ok","timestamp":1620943978114,"user":{"displayName":"Renbin Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDg7JBPfGJg4OqSmd6Xvej6r-SRxhyxgILgsfW=s64","userId":"05750027745545487753"},"user_tz":420},"outputId":"81b8acf2-2bd3-4036-cf4c-dcdbdae7600c"},"source":["batch_size = 8\n","train_set = MTDatasetForBERT(bert_tokenizer, train_src_sent, train_tgt_sent, 1)\n","test_set  = MTDatasetForBERT(bert_tokenizer, test_src_sent, test_tgt_sent, 1)\n","\n","train_data_loader = data.DataLoader(train_set, batch_size=8, num_workers = 8, shuffle=True)\n","test_data_loader = data.DataLoader(test_set, batch_size=1, num_workers = 8, shuffle=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HD15Vk7rONK6","executionInfo":{"elapsed":36576,"status":"ok","timestamp":1620943978115,"user":{"displayName":"Renbin Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDg7JBPfGJg4OqSmd6Xvej6r-SRxhyxgILgsfW=s64","userId":"05750027745545487753"},"user_tz":420},"outputId":"ba6b4452-8f96-4a60-8d80-5ba1a2af39dd"},"source":["print(train_set.max_src_seq_len)\n","print(train_set.max_tgt_seq_len)\n","print(test_set.max_src_seq_len)\n","print(test_set.max_tgt_seq_len)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["201\n","201\n","201\n","201\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aOjCXj98c8Qc"},"source":["# Model Code"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"clYnI32D-K3f"},"source":["# Masking functions\n","def subsequent_mask(size):\n","    attn_shape = (1, size, size)\n","    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n","    return torch.from_numpy(subsequent_mask) == 0\n","\n","def make_std_mask(tgt, pad):\n","    \"Create a mask to hide padding and future words.\"\n","    tgt_mask = (tgt != pad).unsqueeze(-2)\n","    tgt_mask = tgt_mask & Variable(\n","        subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n","    return tgt_mask"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w4pLpGO4dTrF"},"source":["# Loss function/perplexity\n","import math\n","import time\n","\n","class SimpleLossCompute:\n","  \"\"\"A simple loss compute and train function.\"\"\"\n","\n","  def __init__(self, generator, criterion, opt=None):\n","    self.generator = generator\n","    self.criterion = criterion\n","    self.opt = opt\n","\n","  def __call__(self, x, y, norm):\n","    x = self.generator(x)\n","    loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n","                          y.contiguous().view(-1))\n","    loss = loss / norm\n","\n","    if self.opt is not None:  # training mode\n","      loss.backward()          \n","      self.opt.step()\n","      self.opt.zero_grad()\n","\n","    return loss.data.item() * norm\n","\n","def run_epoch(data_loader, model, loss_compute, print_every = 10):\n","    \"Standard Training and Logging Function\"\n","    criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=train_set.PAD)\n","    start = time.time()\n","    total_tokens = 0\n","    total_loss = 0\n","    tokens = 0\n","    for i, batch in enumerate(data_loader):\n","        srcs, src_lens, tgts, tgt_lens = batch\n","        \n","        srcs_mask = srcs.unsqueeze(-2) != train_set.PAD\n","        tgts_mask = make_std_mask(tgts[:,:-1], train_set.PAD)\n","\n","        del tgt_lens\n","\n","        # print('source ids shape', srcs.shape)\n","        # print('target ids shape', tgts[:,:-1].shape)\n","        # print('source masks shape', srcs_mask.shape)\n","        # print('target masks shape', tgts_mask.shape)\n","\n","        out = model.forward(srcs.to(device), tgts[:,:-1].to(device), srcs_mask.to(device), tgts_mask.to(device))\n","        # print(out.size()) # seq_len = 47\n","        # print(tgts[:,1:].size()) # (bs x 46 x ...)\n","\n","        loss = loss_compute(out, tgts[:,1:].to(device), norm=srcs.size(0))\n","        total_loss += loss\n","        total_tokens += (tgts[:,1:] != train_set.PAD).data.sum().item()\n","        \n","        if model.training and i % print_every == 0:\n","          print(\"Epoch Step: %d Loss: %f\" % (i, loss / srcs.size(0)))\n","          print(\"Epoch Step: %d Perplexity: %f\" % (i, math.exp(total_loss / float(total_tokens))))\n","\n","    return math.exp(total_loss / float(total_tokens))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ffbI4KafdbiY"},"source":["def train(model, num_epochs, learning_rate, print_every):\n","  # Set `ignore_index` as PAD_INDEX so that pad tokens won't be included when\n","  # computing the loss.\n","  criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=train_set.PAD)\n","  optim = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.98), eps=1e-9)\n","\n","  # Keep track of dev ppl for each epoch.\n","  dev_ppls = []\n","\n","  for epoch in range(num_epochs):\n","    print(\"Epoch\", epoch)\n","\n","    model.train()\n","    train_ppl = run_epoch(data_loader=train_data_loader, model=model,\n","                          loss_compute=SimpleLossCompute(model.generator,\n","                                                         criterion, optim),\n","                          print_every=print_every)\n","        \n","    model.eval()\n","    with torch.no_grad():      \n","      dev_ppl = run_epoch(data_loader=test_data_loader, model=model,\n","                          loss_compute=SimpleLossCompute(model.generator,\n","                                                         criterion, None),\n","                          print_every=print_every)\n","      print(\"Validation perplexity: %f\" % dev_ppl)\n","      dev_ppls.append(dev_ppl)\n","\n","  # torch.save(model, 'drive/My Drive/MIT/6.864/6.864 FP/transformer/model.pt')\n","        \n","  return dev_ppls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LIuaRumsdcCM"},"source":["# Greedy decode algorithm:\n","def greedy_decode(model, src, src_mask, max_len, start_symbol):\n","    # print(src, src_mask)\n","    encoder_out = model.encode(src, src_mask)\n","    # print(encoder_out)\n","    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n","    \n","    output = []\n","    for i in range(max_len-1):\n","        out = model.decode(ys, encoder_out, src_mask, subsequent_mask(ys.size(1)).type_as(src.data))\n","        prob = model.generator(out[:, -1])\n","        # print(prob)\n","        _, next_word = torch.max(prob, dim = 1)\n","        next_word = next_word.data[0]\n","        if next_word == train_set.EOS: \n","            break\n","        output.append(next_word)    \n","        ys = torch.cat([ys, \n","                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n","    \n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1_d9OWXDd43_"},"source":["# Model Implementation"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uxGMMgEbd6ZZ"},"source":["from torch.autograd import Variable\n","\n","class PositionalEncoder(nn.Module):\n","  def __init__(self, d_model, max_seq_len=201):\n","    super().__init__()\n","    self.d_model = d_model\n","        \n","    # create constant 'pe' matrix with values dependant on \n","    # pos and i\n","    pe = torch.zeros(max_seq_len, d_model)\n","    for pos in range(max_seq_len):\n","      for i in range(0, d_model, 2):\n","        pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n","        pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n","\n","    pe = pe.unsqueeze(0)\n","    self.register_buffer('pe', pe)\n","\n","  def forward(self, x):\n","    x = x * math.sqrt(self.d_model) # conflict possible, check this line\n","\n","    seq_len = x.size(1)\n","    x = x + Variable(self.pe[:,:seq_len], requires_grad=False)# .cuda()\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pd2buCGQd-B3"},"source":["def attention(q, k, v, d_k, mask=None, dropout=None):\n","  scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n","\n","  if mask is not None:\n","    # print('mask size ', mask.size())\n","    # print('q size ', q.size())\n","    # print(scores.size())\n","    mask = mask.unsqueeze(1)\n","    # print(mask.size())\n","    scores = scores.masked_fill(mask==0, -1e9) # approximation\n","\n","  scores = F.softmax(scores, dim=-1)\n","\n","  if dropout is not None:\n","    scores = dropout(scores)\n","\n","  return torch.matmul(scores, v)\n","\n","class MultiHeadAttention(nn.Module):\n","  def __init__(self, heads, d_model, dropout = 0.1):\n","    super().__init__()\n","    self.d_model = d_model\n","    self.d_k = d_model // heads\n","    self.heads = heads\n","\n","    self.query = nn.Linear(d_model, d_model)\n","    self.key = nn.Linear(d_model, d_model)\n","    self.value = nn.Linear(d_model, d_model)\n","    self.dropout = nn.Dropout(dropout)\n","    self.out = nn.Linear(d_model, d_model)\n","\n","  def forward(self, q, k, v, mask=None):\n","    bs = q.size(0)\n","\n","    q = self.query(q).view(bs, -1, self.heads, self.d_k)\n","    k = self.key(k).view(bs, -1, self.heads, self.d_k)\n","    v = self.value(v).view(bs, -1, self.heads, self.d_k)\n","\n","    q = q.transpose(1, 2)\n","    k = k.transpose(1, 2)\n","    v = v.transpose(1, 2)\n","    scores = attention(q, k, v, self.d_k, mask, self.dropout)\n","\n","    concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n","\n","    return self.out(concat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KWeA0WyceCMc"},"source":["class FeedForward(nn.Module):\n","  def __init__(self, d_model, d_ff=2048, dropout=0.1):\n","    super().__init__()\n","\n","    self.l1 = nn.Linear(d_model, d_ff)\n","    self.dropout = nn.Dropout(dropout)\n","    self.l2 = nn.Linear(d_ff, d_model)\n","\n","  def forward(self, x):\n","    x = self.dropout(F.relu(self.l1(x)))\n","    return self.l2(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-OAPS__teFHr"},"source":["class LayerNorm(nn.Module):\n","  def __init__(self, d_model, eps=1e-6):\n","    super().__init__()\n","\n","    self.size = d_model\n","\n","    self.alpha = nn.Parameter(torch.ones(self.size))\n","    self.bias = nn.Parameter(torch.zeros(self.size))\n","    self.eps = eps\n","\n","  def forward(self, x):\n","    norm = self.alpha * (x - x.mean(dim=-1, keepdim=True)) / (x.std(dim=-1, keepdim=True) + self.eps) + self.bias\n","    return norm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wKSsjumUeKnT"},"source":["class EncoderLayer(nn.Module):\n","  def __init__(self, d_model, heads, d_ff = 2048, dropout=0.1):\n","    super().__init__()\n","    self.ln1 = LayerNorm(d_model)\n","    self.ln2 = LayerNorm(d_model)\n","    self.attn = MultiHeadAttention(heads, d_model, dropout)\n","    self.ff = FeedForward(d_model, d_ff, dropout)\n","    self.d1 = nn.Dropout(dropout)\n","    self.d2 = nn.Dropout(dropout)\n","\n","  def forward(self, x, mask):\n","    x2 = self.ln1(x)\n","    x = x + self.d1(self.attn(x2, x2, x2, mask))\n","    x2 = self.ln2(x)\n","    x = x + self.d2(self.ff(x2))\n","    return x\n","\n","class DecoderLayer(nn.Module):\n","  def __init__(self, d_model, heads, d_ff = 2048, dropout=0.1):\n","    super().__init__()\n","    self.ln1 = LayerNorm(d_model)\n","    self.ln2 = LayerNorm(d_model)\n","    self.ln3 = LayerNorm(d_model)\n","\n","    self.d1 = nn.Dropout(dropout)\n","    self.d2 = nn.Dropout(dropout)\n","    self.d3 = nn.Dropout(dropout)\n","\n","    self.attn1 = MultiHeadAttention(heads, d_model, dropout)\n","    self.attn2 = MultiHeadAttention(heads, d_model, dropout)\n","    self.ff = FeedForward(d_model, d_ff, dropout)\n","  \n","  def forward(self, x, encoder_out, src_mask, tgt_mask):\n","    x2 = self.ln1(x)\n","    # print('x2 shape ', x2.shape)\n","    x = x + self.d1(self.attn1(x2, x2, x2, tgt_mask))\n","    x2 = self.ln2(x)\n","    x = x + self.d2(self.attn2(x2, encoder_out, encoder_out, src_mask))\n","    x2 = self.ln3(x)\n","    return x + self.d3(self.ff(x2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VfW_9CIieOLw"},"source":["import copy\n","\n","def clone(module, N):\n","  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n","\n","class Encoder(nn.Module):\n","  def __init__(self, cbert, vocab_size, d_model, N, heads, d_ff = 2048, dropout=0.1):\n","    super().__init__()\n","    self.N = N\n","    self.cbert = cbert\n","    self.pe = PositionalEncoder(d_model)\n","    self.layers = clone(EncoderLayer(d_model, heads, d_ff, dropout), self.N)\n","    self.ln = LayerNorm(d_model)\n","\n","    # self.cbert.resize_token_embeddings(vocab_size)\n","\n","  def forward(self, src, mask):\n","    x = self.cbert(src, mask).last_hidden_state.to(device)\n","    # print('Post BERT: ', x)\n","    x = self.pe(x)\n","    for i in range(self.N):\n","      x = self.layers[i](x, mask)\n","\n","    # print('Post Encoder: ', x)\n","    return self.ln(x)\n","\n","class Decoder(nn.Module):\n","  def __init__(self, vocab_size, d_model, N, heads, d_ff = 2048, dropout=0.1):\n","    super().__init__()\n","    self.N = N\n","    # self.cbert = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n","    self.embed = nn.Embedding(vocab_size, d_model)\n","    self.pe = PositionalEncoder(d_model)\n","    self.layers = clone(DecoderLayer(d_model, heads, d_ff, dropout), self.N)\n","    self.ln = LayerNorm(d_model)\n","\n","    # self.cbert.resize_token_embeddings(vocab_size)\n","\n","  def forward(self, tgt, encoder_out, src_mask, tgt_mask):\n","    x = self.embed(tgt)\n","    x = self.pe(x)\n","    for i in range(self.N):\n","      x = self.layers[i](x, encoder_out, src_mask, tgt_mask)\n","    return self.ln(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q4eUfGOwePbN"},"source":["class Generator(nn.Module):\n","  def __init__(self, d_model, vocab_size):\n","    super().__init__()\n","    self.proj = nn.Linear(d_model, vocab_size)\n","\n","  def forward(self, x):\n","    return F.log_softmax(self.proj(x), dim=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ALZ5NRQIeXmQ"},"source":["# Full Model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BML_jRYieddQ"},"source":["class ClinicalTextTranslationModel(nn.Module):\n","    \"\"\" Model class wrapping encoder + intermediate layers + decoder \"\"\"\n","    def __init__(self, cbert, src_vocab_size, tgt_vocab_size, d_model, N, heads, d_ff = 2048, dropout=0.1, intermediate_layers=None):\n","        \n","        #self, hidden_size, num_heads, N, ff_size, dropout, max_seq_len,\n","        #         intermediate_layers, src_vocab_size, tgt_vocab_size):\n","        \n","        super().__init__()\n","        \n","        self.encoder = Encoder(cbert, src_vocab_size, d_model, N, heads, d_ff = 2048, dropout=0.1)\n","        self.intermediate_layers = intermediate_layers\n","        self.decoder = Decoder(tgt_vocab_size, d_model, N, heads, d_ff = 2048, dropout=0.1)\n","        \n","        self.generator = Generator(d_model, tgt_vocab_size)\n","    \n","    def forward(self, src, tgt, src_mask, tgt_mask):\n","        encoder_output = self.encoder(src, src_mask)\n","        \n","        intermediate_layer_output = encoder_output\n","        \n","        if self.intermediate_layers is not None:\n","            intermediate_layer_output = self.intermediate_layers(intermediate_output)\n","            \n","        decoder_output = self.decoder(tgt, intermediate_layer_output, src_mask, tgt_mask)\n","        \n","        return decoder_output\n","    \n","    def encode(self, src, src_mask):\n","        encoder_output = self.encoder(src, src_mask)\n","        \n","        intermediate_layer_output = encoder_output\n","        \n","        if self.intermediate_layers is not None:\n","            intermediate_layer_output = self.intermediate_layers(intermediate_output)\n","        \n","        return intermediate_layer_output\n","        \n","    def decode(self, tgt, intermediate_layer_output, src_mask,  tgt_mask):\n","        decoder_output = self.decoder(tgt, intermediate_layer_output, src_mask, tgt_mask)\n","        return decoder_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":142,"referenced_widgets":["a25e878072da484981e4e0d561b742d1","6176f70d2c6f448c9ef14534f4e107a4","0a5611343f9544c09d0ec6eed12928dc","d9f7774ce798415699d3689964974811","42f23081243447558b7bf8489476487a","773f3e21f67c49c08835032f41666c18","39fa0b85dc014982834974463004c2f3","213bb7ef2b38447c83a2abe32098dc05"]},"id":"-z6dE68sfrBp","executionInfo":{"elapsed":54681,"status":"ok","timestamp":1620943996336,"user":{"displayName":"Renbin Liu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjDg7JBPfGJg4OqSmd6Xvej6r-SRxhyxgILgsfW=s64","userId":"05750027745545487753"},"user_tz":420},"outputId":"573c8f6d-4e91-4d81-968d-27d87356e307"},"source":["def make_model(src_vocab, tgt_vocab, N=6, \n","               d_model=512, d_ff=2048, h=8, dropout=0.1):\n","    \"Helper: Construct a model from hyperparameters.\"\n","    cbert = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n","    model = ClinicalTextTranslationModel(cbert, src_vocab, tgt_vocab, d_model, N, h, d_ff, dropout).to(device)\n","    \n","    # This was important from their code. \n","    # Initialize parameters with Glorot / fan_avg.\n","    # for p in model.parameters():\n","    #     if p.dim() > 1:\n","    #         nn.init.xavier_uniform(p)\n","    return model\n","\n","init = True\n","if init:\n","  model = make_model(len(bert_tokenizer), len(bert_tokenizer), N=6, d_model=768, d_ff=2048, h=8, dropout=0.1)\n","else:\n","  model = torch.load(\"/content/drive/MyDrive/MedLane/model_6.pt\").to(device)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a25e878072da484981e4e0d561b742d1","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=435778770.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n","- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"b4iXoRxif9hK","outputId":"4fe872d2-05f6-4c4b-801f-b6d91cbaabd1"},"source":["ppls = train(model, 12, 0.00005, 100)\n","print(ppls)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch Step: 0 Loss: 352.617157\n","Epoch Step: 0 Perplexity: 34472.451005\n","Epoch Step: 100 Loss: 206.141479\n","Epoch Step: 100 Perplexity: 10760.198910\n","Epoch Step: 200 Loss: 141.190628\n","Epoch Step: 200 Perplexity: 3639.367677\n","Epoch Step: 300 Loss: 131.298752\n","Epoch Step: 300 Perplexity: 2108.836234\n","Epoch Step: 400 Loss: 169.679840\n","Epoch Step: 400 Perplexity: 1532.777704\n","Epoch Step: 500 Loss: 222.068893\n","Epoch Step: 500 Perplexity: 1208.759905\n","Epoch Step: 600 Loss: 117.369911\n","Epoch Step: 600 Perplexity: 1009.975149\n","Epoch Step: 700 Loss: 140.370697\n","Epoch Step: 700 Perplexity: 864.868809\n","Epoch Step: 800 Loss: 218.467316\n","Epoch Step: 800 Perplexity: 759.408223\n","Epoch Step: 900 Loss: 235.337097\n","Epoch Step: 900 Perplexity: 683.279297\n","Epoch Step: 1000 Loss: 195.413208\n","Epoch Step: 1000 Perplexity: 619.613018\n","Epoch Step: 1100 Loss: 107.790413\n","Epoch Step: 1100 Perplexity: 566.099492\n","Epoch Step: 1200 Loss: 140.070526\n","Epoch Step: 1200 Perplexity: 521.314546\n","Epoch Step: 1300 Loss: 139.749374\n","Epoch Step: 1300 Perplexity: 484.562951\n","Epoch Step: 1400 Loss: 149.470673\n","Epoch Step: 1400 Perplexity: 452.524084\n","Epoch Step: 1500 Loss: 139.647156\n","Epoch Step: 1500 Perplexity: 425.179623\n","Epoch Step: 1600 Loss: 125.429886\n","Epoch Step: 1600 Perplexity: 399.721188\n","Validation perplexity: 275.561602\n","Epoch 1\n","Epoch Step: 0 Loss: 132.325302\n","Epoch Step: 0 Perplexity: 82.338879\n","Epoch Step: 100 Loss: 172.280548\n","Epoch Step: 100 Perplexity: 134.909006\n","Epoch Step: 200 Loss: 94.586121\n","Epoch Step: 200 Perplexity: 130.561216\n","Epoch Step: 300 Loss: 105.727722\n","Epoch Step: 300 Perplexity: 126.439983\n","Epoch Step: 400 Loss: 156.508896\n","Epoch Step: 400 Perplexity: 124.147451\n","Epoch Step: 500 Loss: 101.837021\n","Epoch Step: 500 Perplexity: 121.325040\n","Epoch Step: 600 Loss: 104.635040\n","Epoch Step: 600 Perplexity: 119.054563\n","Epoch Step: 700 Loss: 137.539276\n","Epoch Step: 700 Perplexity: 116.685592\n","Epoch Step: 800 Loss: 96.715233\n","Epoch Step: 800 Perplexity: 113.361698\n","Epoch Step: 900 Loss: 122.886871\n","Epoch Step: 900 Perplexity: 110.978637\n","Epoch Step: 1000 Loss: 132.003937\n","Epoch Step: 1000 Perplexity: 108.379388\n","Epoch Step: 1100 Loss: 145.632797\n","Epoch Step: 1100 Perplexity: 106.637729\n","Epoch Step: 1200 Loss: 104.862495\n","Epoch Step: 1200 Perplexity: 104.222217\n","Epoch Step: 1300 Loss: 144.660141\n","Epoch Step: 1300 Perplexity: 101.627687\n","Epoch Step: 1400 Loss: 185.354324\n","Epoch Step: 1400 Perplexity: 99.256833\n","Epoch Step: 1500 Loss: 142.857422\n","Epoch Step: 1500 Perplexity: 96.971797\n","Epoch Step: 1600 Loss: 208.888153\n","Epoch Step: 1600 Perplexity: 94.991239\n","Validation perplexity: 139.862643\n","Epoch 2\n","Epoch Step: 0 Loss: 113.158173\n","Epoch Step: 0 Perplexity: 62.404156\n","Epoch Step: 100 Loss: 97.055954\n","Epoch Step: 100 Perplexity: 60.385216\n","Epoch Step: 200 Loss: 188.480896\n","Epoch Step: 200 Perplexity: 58.712342\n","Epoch Step: 300 Loss: 162.410355\n","Epoch Step: 300 Perplexity: 57.421669\n","Epoch Step: 400 Loss: 105.201118\n","Epoch Step: 400 Perplexity: 55.497677\n","Epoch Step: 500 Loss: 136.263306\n","Epoch Step: 500 Perplexity: 54.430695\n","Epoch Step: 600 Loss: 104.514992\n","Epoch Step: 600 Perplexity: 53.895413\n","Epoch Step: 700 Loss: 112.349495\n","Epoch Step: 700 Perplexity: 52.738310\n","Epoch Step: 800 Loss: 88.144035\n","Epoch Step: 800 Perplexity: 51.568706\n","Epoch Step: 900 Loss: 123.593857\n","Epoch Step: 900 Perplexity: 50.283914\n","Epoch Step: 1000 Loss: 70.607079\n","Epoch Step: 1000 Perplexity: 49.098690\n","Epoch Step: 1100 Loss: 111.512604\n","Epoch Step: 1100 Perplexity: 48.383266\n","Epoch Step: 1200 Loss: 141.389923\n","Epoch Step: 1200 Perplexity: 47.458078\n","Epoch Step: 1300 Loss: 88.256989\n","Epoch Step: 1300 Perplexity: 46.299962\n","Epoch Step: 1400 Loss: 142.663513\n","Epoch Step: 1400 Perplexity: 45.443511\n","Epoch Step: 1500 Loss: 145.602921\n","Epoch Step: 1500 Perplexity: 44.477390\n","Epoch Step: 1600 Loss: 76.280342\n","Epoch Step: 1600 Perplexity: 43.417748\n","Validation perplexity: 77.305188\n","Epoch 3\n","Epoch Step: 0 Loss: 79.224136\n","Epoch Step: 0 Perplexity: 27.613049\n","Epoch Step: 100 Loss: 126.394989\n","Epoch Step: 100 Perplexity: 28.285632\n","Epoch Step: 200 Loss: 73.531288\n","Epoch Step: 200 Perplexity: 28.136006\n","Epoch Step: 300 Loss: 76.385597\n","Epoch Step: 300 Perplexity: 27.263704\n","Epoch Step: 400 Loss: 89.403099\n","Epoch Step: 400 Perplexity: 26.543147\n","Epoch Step: 500 Loss: 101.236549\n","Epoch Step: 500 Perplexity: 26.202165\n","Epoch Step: 600 Loss: 83.953697\n","Epoch Step: 600 Perplexity: 25.607530\n","Epoch Step: 700 Loss: 85.827866\n","Epoch Step: 700 Perplexity: 25.036840\n","Epoch Step: 800 Loss: 99.175293\n","Epoch Step: 800 Perplexity: 24.540978\n","Epoch Step: 900 Loss: 67.940338\n","Epoch Step: 900 Perplexity: 24.079202\n","Epoch Step: 1000 Loss: 83.549072\n","Epoch Step: 1000 Perplexity: 23.591037\n","Epoch Step: 1100 Loss: 56.663559\n","Epoch Step: 1100 Perplexity: 23.199808\n","Epoch Step: 1200 Loss: 104.133041\n","Epoch Step: 1200 Perplexity: 22.691226\n","Epoch Step: 1300 Loss: 84.077461\n","Epoch Step: 1300 Perplexity: 22.149942\n","Epoch Step: 1400 Loss: 82.569107\n","Epoch Step: 1400 Perplexity: 21.731048\n","Epoch Step: 1500 Loss: 51.503288\n","Epoch Step: 1500 Perplexity: 21.241705\n","Epoch Step: 1600 Loss: 74.540977\n","Epoch Step: 1600 Perplexity: 20.875950\n","Validation perplexity: 44.795185\n","Epoch 4\n","Epoch Step: 0 Loss: 54.913013\n","Epoch Step: 0 Perplexity: 10.886487\n","Epoch Step: 100 Loss: 58.207474\n","Epoch Step: 100 Perplexity: 14.213418\n","Epoch Step: 200 Loss: 90.785767\n","Epoch Step: 200 Perplexity: 13.313902\n","Epoch Step: 300 Loss: 79.061630\n","Epoch Step: 300 Perplexity: 13.215723\n","Epoch Step: 400 Loss: 74.642937\n","Epoch Step: 400 Perplexity: 12.882643\n","Epoch Step: 500 Loss: 62.029675\n","Epoch Step: 500 Perplexity: 12.691010\n","Epoch Step: 600 Loss: 50.779816\n","Epoch Step: 600 Perplexity: 12.655720\n","Epoch Step: 700 Loss: 61.328514\n","Epoch Step: 700 Perplexity: 12.617606\n","Epoch Step: 800 Loss: 62.432873\n","Epoch Step: 800 Perplexity: 12.374134\n","Epoch Step: 900 Loss: 101.550026\n","Epoch Step: 900 Perplexity: 12.317322\n","Epoch Step: 1000 Loss: 56.273605\n","Epoch Step: 1000 Perplexity: 12.272842\n","Epoch Step: 1100 Loss: 59.262753\n","Epoch Step: 1100 Perplexity: 12.114144\n","Epoch Step: 1200 Loss: 56.996883\n","Epoch Step: 1200 Perplexity: 11.896392\n","Epoch Step: 1300 Loss: 56.464893\n","Epoch Step: 1300 Perplexity: 11.769368\n","Epoch Step: 1400 Loss: 74.035431\n","Epoch Step: 1400 Perplexity: 11.585557\n","Epoch Step: 1500 Loss: 76.923851\n","Epoch Step: 1500 Perplexity: 11.433248\n","Epoch Step: 1600 Loss: 117.513916\n","Epoch Step: 1600 Perplexity: 11.272623\n","Validation perplexity: 30.221779\n","Epoch 5\n","Epoch Step: 0 Loss: 94.257561\n","Epoch Step: 0 Perplexity: 13.466546\n","Epoch Step: 100 Loss: 66.455536\n","Epoch Step: 100 Perplexity: 8.642512\n","Epoch Step: 200 Loss: 53.341423\n","Epoch Step: 200 Perplexity: 8.352057\n","Epoch Step: 300 Loss: 45.302406\n","Epoch Step: 300 Perplexity: 8.301124\n","Epoch Step: 400 Loss: 69.591644\n","Epoch Step: 400 Perplexity: 8.151182\n","Epoch Step: 500 Loss: 44.421879\n","Epoch Step: 500 Perplexity: 8.130971\n","Epoch Step: 600 Loss: 45.124664\n","Epoch Step: 600 Perplexity: 7.981361\n","Epoch Step: 700 Loss: 77.877327\n","Epoch Step: 700 Perplexity: 7.864514\n","Epoch Step: 800 Loss: 85.814461\n","Epoch Step: 800 Perplexity: 7.742692\n","Epoch Step: 900 Loss: 84.412163\n","Epoch Step: 900 Perplexity: 7.704762\n","Epoch Step: 1000 Loss: 79.279228\n","Epoch Step: 1000 Perplexity: 7.667345\n","Epoch Step: 1100 Loss: 54.829376\n","Epoch Step: 1100 Perplexity: 7.571992\n","Epoch Step: 1200 Loss: 39.752033\n","Epoch Step: 1200 Perplexity: 7.468219\n","Epoch Step: 1300 Loss: 71.962372\n","Epoch Step: 1300 Perplexity: 7.453258\n","Epoch Step: 1400 Loss: 36.761379\n","Epoch Step: 1400 Perplexity: 7.396340\n","Epoch Step: 1500 Loss: 46.928684\n","Epoch Step: 1500 Perplexity: 7.336944\n","Epoch Step: 1600 Loss: 32.116959\n","Epoch Step: 1600 Perplexity: 7.250838\n","Validation perplexity: 22.859782\n","Epoch 6\n","Epoch Step: 0 Loss: 33.083317\n","Epoch Step: 0 Perplexity: 3.367150\n","Epoch Step: 100 Loss: 90.871490\n","Epoch Step: 100 Perplexity: 5.999478\n","Epoch Step: 200 Loss: 41.881763\n","Epoch Step: 200 Perplexity: 5.950951\n","Epoch Step: 300 Loss: 39.279602\n","Epoch Step: 300 Perplexity: 5.878501\n","Epoch Step: 400 Loss: 55.192375\n","Epoch Step: 400 Perplexity: 5.783844\n","Epoch Step: 500 Loss: 80.801079\n","Epoch Step: 500 Perplexity: 5.641129\n","Epoch Step: 600 Loss: 65.045609\n","Epoch Step: 600 Perplexity: 5.573179\n","Epoch Step: 700 Loss: 63.340721\n","Epoch Step: 700 Perplexity: 5.517878\n","Epoch Step: 800 Loss: 64.674934\n","Epoch Step: 800 Perplexity: 5.496708\n","Epoch Step: 900 Loss: 40.555630\n","Epoch Step: 900 Perplexity: 5.499586\n","Epoch Step: 1000 Loss: 72.492546\n","Epoch Step: 1000 Perplexity: 5.466505\n","Epoch Step: 1100 Loss: 39.706039\n","Epoch Step: 1100 Perplexity: 5.426039\n","Epoch Step: 1200 Loss: 76.814613\n","Epoch Step: 1200 Perplexity: 5.432491\n","Epoch Step: 1300 Loss: 51.565487\n","Epoch Step: 1300 Perplexity: 5.400503\n","Epoch Step: 1400 Loss: 46.666676\n","Epoch Step: 1400 Perplexity: 5.355922\n","Epoch Step: 1500 Loss: 36.461880\n","Epoch Step: 1500 Perplexity: 5.329974\n","Epoch Step: 1600 Loss: 15.578887\n","Epoch Step: 1600 Perplexity: 5.306923\n","Validation perplexity: 20.498489\n","Epoch 7\n","Epoch Step: 0 Loss: 46.668922\n","Epoch Step: 0 Perplexity: 3.560506\n","Epoch Step: 100 Loss: 29.326420\n","Epoch Step: 100 Perplexity: 4.586399\n","Epoch Step: 200 Loss: 29.327751\n","Epoch Step: 200 Perplexity: 4.437093\n","Epoch Step: 300 Loss: 35.982052\n","Epoch Step: 300 Perplexity: 4.471918\n","Epoch Step: 400 Loss: 37.098919\n","Epoch Step: 400 Perplexity: 4.477386\n","Epoch Step: 500 Loss: 52.153667\n","Epoch Step: 500 Perplexity: 4.397960\n","Epoch Step: 600 Loss: 39.047089\n","Epoch Step: 600 Perplexity: 4.371456\n","Epoch Step: 700 Loss: 36.115040\n","Epoch Step: 700 Perplexity: 4.345867\n","Epoch Step: 800 Loss: 41.006954\n","Epoch Step: 800 Perplexity: 4.333840\n","Epoch Step: 900 Loss: 38.258701\n","Epoch Step: 900 Perplexity: 4.322294\n","Epoch Step: 1000 Loss: 66.634506\n","Epoch Step: 1000 Perplexity: 4.305714\n","Epoch Step: 1100 Loss: 53.271015\n","Epoch Step: 1100 Perplexity: 4.274698\n","Epoch Step: 1200 Loss: 38.140240\n","Epoch Step: 1200 Perplexity: 4.258130\n","Epoch Step: 1300 Loss: 38.219997\n","Epoch Step: 1300 Perplexity: 4.243699\n","Epoch Step: 1400 Loss: 25.556263\n","Epoch Step: 1400 Perplexity: 4.226688\n","Epoch Step: 1500 Loss: 63.659573\n","Epoch Step: 1500 Perplexity: 4.216722\n","Epoch Step: 1600 Loss: 20.884800\n","Epoch Step: 1600 Perplexity: 4.211434\n","Validation perplexity: 17.687552\n","Epoch 8\n","Epoch Step: 0 Loss: 26.807566\n","Epoch Step: 0 Perplexity: 3.313826\n","Epoch Step: 100 Loss: 60.707184\n","Epoch Step: 100 Perplexity: 3.421117\n","Epoch Step: 200 Loss: 50.550552\n","Epoch Step: 200 Perplexity: 3.379462\n","Epoch Step: 300 Loss: 24.002533\n","Epoch Step: 300 Perplexity: 3.457113\n","Epoch Step: 400 Loss: 63.424004\n","Epoch Step: 400 Perplexity: 3.520922\n","Epoch Step: 500 Loss: 53.386890\n","Epoch Step: 500 Perplexity: 3.521215\n","Epoch Step: 600 Loss: 62.751404\n","Epoch Step: 600 Perplexity: 3.561890\n","Epoch Step: 700 Loss: 28.853945\n","Epoch Step: 700 Perplexity: 3.545341\n","Epoch Step: 800 Loss: 26.584406\n","Epoch Step: 800 Perplexity: 3.553996\n","Epoch Step: 900 Loss: 31.977333\n","Epoch Step: 900 Perplexity: 3.530554\n","Epoch Step: 1000 Loss: 28.446827\n","Epoch Step: 1000 Perplexity: 3.529456\n","Epoch Step: 1100 Loss: 37.830887\n","Epoch Step: 1100 Perplexity: 3.531309\n","Epoch Step: 1200 Loss: 55.783806\n","Epoch Step: 1200 Perplexity: 3.525928\n","Epoch Step: 1300 Loss: 32.121990\n","Epoch Step: 1300 Perplexity: 3.526688\n","Epoch Step: 1400 Loss: 21.975931\n","Epoch Step: 1400 Perplexity: 3.528974\n","Epoch Step: 1500 Loss: 49.613892\n","Epoch Step: 1500 Perplexity: 3.534570\n","Epoch Step: 1600 Loss: 12.904053\n","Epoch Step: 1600 Perplexity: 3.524384\n","Validation perplexity: 16.020054\n","Epoch 9\n","Epoch Step: 0 Loss: 56.125599\n","Epoch Step: 0 Perplexity: 4.122331\n","Epoch Step: 100 Loss: 32.479843\n","Epoch Step: 100 Perplexity: 3.155285\n","Epoch Step: 200 Loss: 33.094204\n","Epoch Step: 200 Perplexity: 3.215031\n","Epoch Step: 300 Loss: 21.522429\n","Epoch Step: 300 Perplexity: 3.100905\n","Epoch Step: 400 Loss: 32.821823\n","Epoch Step: 400 Perplexity: 3.060360\n","Epoch Step: 500 Loss: 17.758436\n","Epoch Step: 500 Perplexity: 3.066115\n","Epoch Step: 600 Loss: 20.333778\n","Epoch Step: 600 Perplexity: 3.077485\n","Epoch Step: 700 Loss: 41.421310\n","Epoch Step: 700 Perplexity: 3.054034\n","Epoch Step: 800 Loss: 17.951094\n","Epoch Step: 800 Perplexity: 3.075233\n","Epoch Step: 900 Loss: 49.282940\n","Epoch Step: 900 Perplexity: 3.080281\n","Epoch Step: 1000 Loss: 30.469475\n","Epoch Step: 1000 Perplexity: 3.076809\n","Epoch Step: 1100 Loss: 17.652212\n","Epoch Step: 1100 Perplexity: 3.074605\n","Epoch Step: 1200 Loss: 23.296448\n","Epoch Step: 1200 Perplexity: 3.073363\n","Epoch Step: 1300 Loss: 34.278313\n","Epoch Step: 1300 Perplexity: 3.064871\n","Epoch Step: 1400 Loss: 27.522007\n","Epoch Step: 1400 Perplexity: 3.058379\n","Epoch Step: 1500 Loss: 40.021172\n","Epoch Step: 1500 Perplexity: 3.062294\n","Epoch Step: 1600 Loss: 23.462429\n","Epoch Step: 1600 Perplexity: 3.057932\n","Validation perplexity: 15.086034\n","Epoch 10\n","Epoch Step: 0 Loss: 29.087301\n","Epoch Step: 0 Perplexity: 2.839110\n","Epoch Step: 100 Loss: 49.021549\n","Epoch Step: 100 Perplexity: 2.683060\n","Epoch Step: 200 Loss: 35.100330\n","Epoch Step: 200 Perplexity: 2.707195\n","Epoch Step: 300 Loss: 25.191013\n","Epoch Step: 300 Perplexity: 2.720900\n","Epoch Step: 400 Loss: 43.551868\n","Epoch Step: 400 Perplexity: 2.750155\n","Epoch Step: 500 Loss: 15.815815\n","Epoch Step: 500 Perplexity: 2.743091\n","Epoch Step: 600 Loss: 18.596817\n","Epoch Step: 600 Perplexity: 2.732534\n","Epoch Step: 700 Loss: 51.846310\n","Epoch Step: 700 Perplexity: 2.735161\n","Epoch Step: 800 Loss: 41.615810\n","Epoch Step: 800 Perplexity: 2.737118\n","Epoch Step: 900 Loss: 16.092360\n","Epoch Step: 900 Perplexity: 2.738710\n","Epoch Step: 1000 Loss: 15.845106\n","Epoch Step: 1000 Perplexity: 2.735945\n","Epoch Step: 1100 Loss: 29.730364\n","Epoch Step: 1100 Perplexity: 2.734513\n","Epoch Step: 1200 Loss: 26.685688\n","Epoch Step: 1200 Perplexity: 2.731044\n","Epoch Step: 1300 Loss: 11.658273\n","Epoch Step: 1300 Perplexity: 2.723993\n","Epoch Step: 1400 Loss: 25.225529\n","Epoch Step: 1400 Perplexity: 2.721491\n","Epoch Step: 1500 Loss: 38.349663\n","Epoch Step: 1500 Perplexity: 2.728276\n","Epoch Step: 1600 Loss: 17.696087\n","Epoch Step: 1600 Perplexity: 2.733174\n","Validation perplexity: 14.000308\n","Epoch 11\n","Epoch Step: 0 Loss: 26.747873\n","Epoch Step: 0 Perplexity: 2.644944\n","Epoch Step: 100 Loss: 21.549192\n","Epoch Step: 100 Perplexity: 2.462921\n","Epoch Step: 200 Loss: 21.675196\n","Epoch Step: 200 Perplexity: 2.488473\n","Epoch Step: 300 Loss: 25.094809\n","Epoch Step: 300 Perplexity: 2.460259\n","Epoch Step: 400 Loss: 31.487961\n","Epoch Step: 400 Perplexity: 2.461554\n","Epoch Step: 500 Loss: 31.389381\n","Epoch Step: 500 Perplexity: 2.448327\n","Epoch Step: 600 Loss: 26.231918\n","Epoch Step: 600 Perplexity: 2.475058\n","Epoch Step: 700 Loss: 27.722778\n","Epoch Step: 700 Perplexity: 2.478786\n","Epoch Step: 800 Loss: 32.267334\n","Epoch Step: 800 Perplexity: 2.488141\n","Epoch Step: 900 Loss: 38.323845\n","Epoch Step: 900 Perplexity: 2.500287\n","Epoch Step: 1000 Loss: 25.628775\n","Epoch Step: 1000 Perplexity: 2.482565\n","Epoch Step: 1100 Loss: 45.002811\n","Epoch Step: 1100 Perplexity: 2.495706\n","Epoch Step: 1200 Loss: 15.095846\n","Epoch Step: 1200 Perplexity: 2.495152\n","Epoch Step: 1300 Loss: 33.715424\n","Epoch Step: 1300 Perplexity: 2.491833\n","Epoch Step: 1400 Loss: 21.286327\n","Epoch Step: 1400 Perplexity: 2.489596\n","Epoch Step: 1500 Loss: 13.838569\n","Epoch Step: 1500 Perplexity: 2.485526\n","Epoch Step: 1600 Loss: 56.249840\n","Epoch Step: 1600 Perplexity: 2.491109\n","Validation perplexity: 13.703583\n","[275.56160160485166, 139.86264289062652, 77.30518807457148, 44.795185220796405, 30.22177864764396, 22.859781675380347, 20.498489309570814, 17.687552000330747, 16.020053574942843, 15.0860335704967, 14.000308422777733, 13.703583235289969]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"axHenfVGxO9w"},"source":["# torch.save(model, \"/content/drive/MyDrive/MedLane/model_12.pt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"A4rSOUqNKmqP","outputId":"2e202516-e013-451c-abb8-57c0870aba4d"},"source":["!pip -q install sacrebleu\n","import sacrebleu\n","from tqdm import tqdm\n","\n","def compute_BLEU(model, data_loader, decoder, max_iters):\n","  bleu_scores = []\n","\n","  for i, batch in enumerate(test_data_loader):\n","    if i >= max_iters: break\n","    srcs, src_lens, tgts, tgt_lens = batch\n","          \n","    srcs_mask = srcs.unsqueeze(-2) != train_set.PAD\n","\n","    # src_sent = [bert_tokenizer.convert_ids_to_tokens(token.item()) for token in srcs[0]]\n","    # print('Source sent: ', bert_tokenizer.convert_tokens_to_string(src_sent))    \n","    \n","    out = decoder(model, srcs.to(device), srcs_mask.to(device), 201, train_set.SOS)\n","    # print(tgts[0,:])\n","    tgts = tgts[0,1:]\n","    \n","    tgts = tgts[:np.where(tgts == train_set.EOS)[0][0]]\n","    \n","    tgt_sent = [bert_tokenizer.convert_ids_to_tokens(token.item()) for token in tgts]\n","    out_sent = [bert_tokenizer.convert_ids_to_tokens(token.item()) for token in out]\n","    print('Target Tokens: ', tgt_sent)\n","    print('Out tokens: ', out_sent)\n","\n","    tgt_sent = bert_tokenizer.convert_tokens_to_string(tgt_sent)\n","    out_sent = bert_tokenizer.convert_tokens_to_string(out_sent)\n","\n","    print('Target sent: ', tgt_sent)\n","    print('Out sent: ', out_sent)\n","\n","    bleu_scores.append(sacrebleu.raw_corpus_bleu([out_sent], [[tgt_sent]], .01).score)\n","\n","  return bleu_scores\n","\n","# print('BLEU score: %f' % (np.mean(compute_BLEU(model, \n","#                                            test_data_loader,\n","#                                             greedy_decode))))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[?25l\r\u001b[K     |██████                          | 10kB 32.2MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20kB 38.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30kB 21.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 40kB 16.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 51kB 8.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 5.1MB/s \n","\u001b[?25h"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"L0V5KolZ-Prb"},"source":["# model = torch.load(\"/content/drive/MyDrive/MedLane/model_5.pt\", map_location=torch.device('cpu'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"zEzRNx7lvXaP","outputId":"26b0d7ed-4838-4d25-f4f8-0cc56dde1f79"},"source":["\"\"\"\n","Potential Improvements/Experiments\n"," - Try BERT tokenizers e.g ClinicalBERT\n"," - start with ClincalBERT/BioBERT/BEHRT pretrained embeddings\n"," - Intermediate layers?\n"," - try varying N, adjust hyperparams\n"," - make a validation set from part of the training examples?\n"," - \"Tunability\" - Use only most common words/words with low enough reading scores\n"," - Front-load any UI that you can!\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nPotential Improvements/Experiments\\n - Try BERT tokenizers e.g ClinicalBERT\\n - start with ClincalBERT/BioBERT/BEHRT pretrained embeddings\\n - Intermediate layers?\\n - try varying N, adjust hyperparams\\n - make a validation set from part of the training examples?\\n - \"Tunability\" - Use only most common words/words with low enough reading scores\\n - Front-load any UI that you can!\\n'"]},"metadata":{"tags":[]},"execution_count":0}]},{"cell_type":"code","metadata":{"id":"eAkKWCOICRZL"},"source":["device = 'cuda'\n","bleu_scores = compute_BLEU(model.to(device), test_data_loader, greedy_decode, 25000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"5ohyzYOrGjWY"},"source":["# transformer = torch.load('/content/drive/MyDrive/MIT/6.871/6.871 NLP Project/data/model_basic.pt', map_location=torch.device('cpu'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tQZYzbuqG0Tg"},"source":["np.mean(bleu_scores)"],"execution_count":null,"outputs":[]}]}